{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCSpvVOx8r/wD5M3zN89C4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/LLMs-Journey/blob/main/ch1/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Word Embeddings for Text Data"
      ],
      "metadata": {
        "id": "GEMGG4OdKjgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores the concept of word embeddings, which are essential for processing raw text in deep neural networks and language models. We’ll go through the theory of word embeddings, discuss why they're necessary, and implement a basic Word2Vec model to generate embeddings.\n"
      ],
      "metadata": {
        "id": "0qYLwTqlKnlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to Word Embeddings\n",
        "\n",
        "In machine learning, especially deep learning, we need to convert text data into a format that models can process. Words are categorical data, and we need a way to represent them as **continuous numerical vectors**. This transformation process is called embedding.\n",
        "\n",
        "An *embedding* is a mapping from discrete objects (like words) to points in a continuous vector space, where the distance between vectors indicates semantic similarity. The purpose is to convert text into a numerical format that neural networks can understand.\n"
      ],
      "metadata": {
        "id": "CjVDNRPOKs-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Why Neural Networks Need Embeddings\n",
        "\n",
        "Deep learning models cannot process raw text. Since text data is categorical, it isn’t compatible with the mathematical operations required in neural networks. By converting text into embeddings, we create **dense vector representations** that retain semantic information about the data.\n",
        "\n",
        "Different types of embeddings:\n",
        "- **Word embeddings**: Represent individual words.\n",
        "- **Sentence embeddings**: Represent whole sentences.\n",
        "- **Paragraph embeddings**: Represent paragraphs or even documents.\n",
        "\n",
        "For our purposes, we’ll focus on word embeddings, particularly Word2Vec, which learns word representations by predicting word context.\n"
      ],
      "metadata": {
        "id": "BdwVbiaJK_VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gensim and matplotlib if not already installed\n",
        "!pip install gensim matplotlib"
      ],
      "metadata": {
        "id": "P-mL3n33V_JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "OwYYN9dSWAg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the Corpus\n",
        "\n",
        "For demonstration purposes, we'll use the Text8 corpus, which is a popular dataset for training word embeddings."
      ],
      "metadata": {
        "id": "KvLETQLAWOZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Download the Text8 corpus\n",
        "dataset_url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "dataset_path = 'text8.zip'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    response = requests.get(dataset_url)\n",
        "    with open(dataset_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Extract the corpus\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "# Read the corpus\n",
        "with open('text8', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Split into sentences (for simplicity, split by 1000 words)\n",
        "words = data.split()\n",
        "sentences = [words[i:i+1000] for i in range(0, len(words), 1000)]"
      ],
      "metadata": {
        "id": "GIaK0tRSWMG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Word2Vec Model"
      ],
      "metadata": {
        "id": "okaPg5ELcrF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Save the model for future use\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "CJUIQDjPWY1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Embeddings\n"
      ],
      "metadata": {
        "id": "KHdRHdzJcuMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Find most similar words\n",
        "similar_words = model.wv.most_similar('science', topn=10)\n",
        "print(\"Words similar to 'science':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2caMZA_cLlI",
        "outputId": "ac8c81ad-ff7f-4ba8-87b0-4c284a675101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'science':\n",
            "psychology: 0.6781\n",
            "sociology: 0.6348\n",
            "humanities: 0.6293\n",
            "anthropology: 0.6172\n",
            "sciences: 0.6116\n",
            "aesthetics: 0.5952\n",
            "mathematics: 0.5910\n",
            "astronomy: 0.5868\n",
            "memetics: 0.5837\n",
            "engineering: 0.5817\n"
          ]
        }
      ]
    }
  ]
}