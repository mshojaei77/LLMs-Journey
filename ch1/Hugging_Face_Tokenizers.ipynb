{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPikSlfgoSuGhtKzOGh32e2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/NLP-Journey/blob/main/ch1/Hugging_Face_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Tokenizers for Beginners\n",
        "\n",
        "Hugging Face tokenizers are powerful tools used in natural language processing (NLP) to convert text into a format that machine learning models can understand. Let's explore how they work and why they're so useful!\n",
        "\n",
        "## The Concept\n",
        "\n",
        "Imagine you're translating a book into a secret code that a computer can understand. That's essentially what a tokenizer does! It takes text and breaks it down into smaller pieces (tokens) that a machine learning model can process.\n",
        "\n",
        "Hugging Face tokenizers are special because they:\n",
        "1. Are very fast\n",
        "2. Handle various languages and special characters well\n",
        "3. Can be customized for different tasks\n",
        "4. Work seamlessly with popular NLP models\n",
        "\n",
        "Let's see how to use a Hugging Face tokenizer!"
      ],
      "metadata": {
        "id": "mwME2sC49a9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VL8I0GG78wvz"
      },
      "outputs": [],
      "source": [
        "# First, we need to install the transformers library\n",
        "!pip install -q transformers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer (in this case, BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sub> read about bert-base-uncased pre-trained tokenizer model here: https://huggingface.co/google-bert/bert-base-*uncased*</sub>"
      ],
      "metadata": {
        "id": "jjeqe6HvCLV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Tokenizer\n",
        "\n",
        "Now that we have our tokenizer, let's see it in action!"
      ],
      "metadata": {
        "id": "jCoMXZgT_oxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ancient Persia, with its sun-warmed landscapes and rich cultural heritage, was a powerful civilization\"\n",
        "\n",
        "# Tokenize the text\n",
        "output = tokenizer(text)\n",
        "\n",
        "print(\"Tokenized output:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AEWD2jK_pir",
        "outputId": "1d9fc907-5ac2-401e-8b34-113371126496"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized output:\n",
            "{'input_ids': [101, 3418, 16667, 1010, 2007, 2049, 3103, 1011, 17336, 12793, 1998, 4138, 3451, 4348, 1010, 2001, 1037, 3928, 10585, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What just happened?\n",
        "\n",
        "The tokenizer did several things:\n",
        "1. It split the text into tokens\n",
        "2. It converted each token to a unique ID\n",
        "\n",
        "Let's break this down further:"
      ],
      "metadata": {
        "id": "2dmnB4j-ANdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokens\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = tokenizer.decode(input_ids)\n",
        "print(\"Decoded text:\", decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeaoV_TLBV0O",
        "outputId": "1af38b74-104f-4099-b429-ceaa9aadbd0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['ancient', 'persia', ',', 'with', 'its', 'sun', '-', 'warmed', 'landscapes', 'and', 'rich', 'cultural', 'heritage', ',', 'was', 'a', 'powerful', 'civilization']\n",
            "Input IDs: [3418, 16667, 1010, 2007, 2049, 3103, 1011, 17336, 12793, 1998, 4138, 3451, 4348, 1010, 2001, 1037, 3928, 10585]\n",
            "Decoded text: ancient persia, with its sun - warmed landscapes and rich cultural heritage, was a powerful civilization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Features of Hugging Face Tokenizers\n",
        "\n",
        "1. **Subword Tokenization**: Notice how `sun-warmed` was split into `sun`, `-`, and `warmed`. This allows the model to understand parts of words it hasn't seen before.\n",
        "\n",
        "2. **Handling Multiple Sentences**: Let's see how it handles multiple sentences and pads them to the same length."
      ],
      "metadata": {
        "id": "0YHE3woEBCpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Ancient Persia, with its sun-warmed landscapes and rich cultural heritage, was a powerful civilization\",\n",
        "             \"that significantly influenced the development of art and architecture in the Middle East and beyond.\"]\n",
        "\n",
        "# Tokenize with padding\n",
        "padded_output = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Padded output:\")\n",
        "print(padded_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biaReY8bALnA",
        "outputId": "9c833623-86b1-4394-f4e4-319a68ef680b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded output:\n",
            "{'input_ids': tensor([[  101,  3418, 16667,  1010,  2007,  2049,  3103,  1011, 17336, 12793,\n",
            "          1998,  4138,  3451,  4348,  1010,  2001,  1037,  3928, 10585,   102],\n",
            "        [  101,  2008,  6022,  5105,  1996,  2458,  1997,  2396,  1998,  4294,\n",
            "          1999,  1996,  2690,  2264,  1998,  3458,  1012,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "padding=True ensures that all tokenized sequences are of the same length by adding padding tokens (usually zeros) to the shorter sequences. truncation=True limits the length of the tokenized sequences to a predefined maximum length, truncating any tokens beyond this limit. return_tensors=\"pt\" specifies that the output should be in PyTorch tensor format, which is a common format for inputting data into neural networks. This preprocessing step is crucial for batch processing and ensuring that all inputs to the model are consistent in shape and size."
      ],
      "metadata": {
        "id": "84nIQpJCFkYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Attention Masks**: The tokenizer also provides attention masks, which tell the model which tokens are actual words and which are padding.\n",
        "\n",
        "4. **Special Tokens**: Let's look at how special tokens are handled."
      ],
      "metadata": {
        "id": "X0aijt9lFkL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Special tokens:\")\n",
        "print(f\"CLS token: {tokenizer.cls_token}, ID: {tokenizer.cls_token_id}\")\n",
        "print(f\"SEP token: {tokenizer.sep_token}, ID: {tokenizer.sep_token_id}\")\n",
        "print(f\"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GTjrMWgGJ3j",
        "outputId": "59698465-dc5f-4644-8184-8efd5d9555a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens:\n",
            "CLS token: [CLS], ID: 101\n",
            "SEP token: [SEP], ID: 102\n",
            "PAD token: [PAD], ID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special tokens are predefined tokens used in natural language processing models, particularly those based on architectures like BERT, to handle specific tasks or provide essential information during the tokenization process. The [CLS] token, with an ID of 101, is placed at the beginning of each sequence and is used for classification tasks, where the model's output for this token is interpreted as a summary of the entire sequence. The [SEP] token, ID 102, is used to separate different segments of text, such as separating two sentences in a sequence or marking the end of a single sentence. The [PAD] token, ID 0, is used for padding, ensuring that all sequences in a batch have the same length by filling in extra space with this token. These special tokens help the model understand the structure of the input data and perform tasks more effectively."
      ],
      "metadata": {
        "id": "AEZ5jjVHGQDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why are Hugging Face Tokenizers Special?\n",
        "\n",
        "1. **Speed**: They're implemented in Rust, making them very fast.\n",
        "2. **Flexibility**: They can be used with various models and tasks.\n",
        "3. **Pre-training**: They come pre-trained for popular models, saving time and effort.\n",
        "4. **Customization**: You can fine-tune them for specific tasks or domains.\n",
        "\n",
        "## Real-world Applications\n",
        "\n",
        "Hugging Face tokenizers are used in many NLP tasks, including:\n",
        "- Text classification\n",
        "- Named Entity Recognition\n",
        "- Question Answering\n",
        "- Machine Translation\n",
        "\n",
        "They're a crucial part of the pipeline in modern NLP systems."
      ],
      "metadata": {
        "id": "ztpfC4hYG5tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how the tokenizer handles a more complex sentence\n",
        "complex_sentence = \"The year is 2023, and AI is advancing rapidly! 🚀\"\n",
        "complex_output = tokenizer(complex_sentence)\n",
        "\n",
        "print(\"Tokens for complex sentence:\")\n",
        "print(tokenizer.convert_ids_to_tokens(complex_output['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W-oqfDnG04_",
        "outputId": "a86095e1-edc9-4d29-9553-4ec597d3703d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens for complex sentence:\n",
            "['[CLS]', 'the', 'year', 'is', '202', '##3', ',', 'and', 'ai', 'is', 'advancing', 'rapidly', '!', '[UNK]', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the tokenizer handles numbers, punctuation, and even emojis!\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Hugging Face tokenizers are powerful tools that bridge the gap between human-readable text and machine-processable data. They're an essential component in modern NLP pipelines, allowing models to understand and generate human language more effectively.\n",
        "\n",
        "By using pre-trained tokenizers, you can quickly get started with advanced NLP tasks without having to worry about the intricacies of text preprocessing. As you delve deeper into NLP, understanding how these tokenizers work will help you fine-tune your models and tackle more complex language tasks."
      ],
      "metadata": {
        "id": "7pj0hdoZHdws"
      }
    }
  ]
}