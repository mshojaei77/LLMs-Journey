{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtZXycJn8/eZemTZvc47W+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/NLP-Journey/blob/main/ch1/Tokenization_BPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Tokenization\n",
        "Tokenization is a fundamental step in natural language processing (NLP) that involves breaking down text into smaller, meaningful units called tokens. These tokens are then used as input to neural networks for various NLP tasks such as text classification, translation, and generation.\n",
        "\n",
        "## Why is Tokenization Important?\n",
        "Tokenization is crucial because it allows the model to understand and process natural language in a structured way. By breaking down text into smaller units, the model can learn the relationships between words and phrases, which is essential for tasks like language understanding and generation."
      ],
      "metadata": {
        "id": "jTisdAShqrWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whitespace Tokenization\n",
        "\n",
        "Below I am showing you an example of a simple tokenizer without any following any standards. All it does is extract tokens based on a white space seperator.\n",
        "\n",
        "Try to running the following code blocks."
      ],
      "metadata": {
        "id": "xiTZbhoBq-VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Cyrus the Great founded the Achaemenid Empire.\"\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl6GPCCcq91w",
        "outputId": "cf8c9c4d-5c28-4c18-d29c-b42788912c26"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cyrus', 'the', 'Great', 'founded', 'the', 'Achaemenid', 'Empire.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The split() method divides the text at every space, creating tokens of words. This method is straightforward but does not consider punctuation, so \"Empire.\" remains a single token despite its proximity to the period.\n"
      ],
      "metadata": {
        "id": "gQEeNRmosCP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Tokenization Techniques\n",
        "### NLTK Tokenizers\n",
        "NLTK (Natural Language Toolkit) is a popular library in Python for working with human language data. It provides built-in tokenizers for splitting text into sentences or words.\n"
      ],
      "metadata": {
        "id": "lEEm2DJfu3YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Cyrus the Great founded the Achaemenid Empire, It was a powerful empire.\"\n",
        "sent_tokens = sent_tokenize(text)\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Sentence Tokens:\", sent_tokens)\n",
        "print(\"Word Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bWrwWoNqZJB",
        "outputId": "6c11a48e-9e51-4535-b664-0c545eb8034e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Cyrus the Great founded the Achaemenid Empire, It was a powerful empire.']\n",
            "Word Tokens: ['Cyrus', 'the', 'Great', 'founded', 'the', 'Achaemenid', 'Empire', ',', 'It', 'was', 'a', 'powerful', 'empire', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SpaCy Tokenizers\n",
        "SpaCy is another popular NLP library that provides robust tokenization as part of its processing pipeline. It is designed for high performance and ease of use."
      ],
      "metadata": {
        "id": "Scrf54qkxAhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Cyrus the Great founded the Achaemenid Empire, It was a powerful empire.\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cczx1QiqYmZ",
        "outputId": "adca3904-c075-482a-939f-bb323caf5804"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cyrus', 'the', 'Great', 'founded', 'the', 'Achaemenid', 'Empire', '.', 'It', 'was', 'a', 'powerful', 'empire', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subword Tokenization\n",
        "Subword tokenization breaks down words into smaller units. This approach is useful for handling rare words or languages with complex morphology.\n"
      ],
      "metadata": {
        "id": "ahcGQzGJzZWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding (BPE)\n",
        "\n",
        "Byte Pair Encoding is a technique used in natural language processing to break words into subwords. Let's explore how it works using a simple, step-by-step example.\n",
        "\n",
        "## The Concept\n",
        "\n",
        "Imagine you're creating a new alphabet for children learning to read. Instead of just having individual letters, you also create special blocks for common letter pairs or groups. This is essentially what BPE does!\n",
        "\n",
        "## Let's implement a simple version of BPE"
      ],
      "metadata": {
        "id": "kvcS9l9j5Dcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Count frequency of character pairs in the vocabulary\"\"\"\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"Merge the most frequent pair in the vocabulary\"\"\"\n",
        "    v_out = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    for word in v_in:\n",
        "        w_out = word.replace(bigram, replacement)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "g_xGjUCV1Yco"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Starting Point\n",
        "\n",
        "Let's begin with a simple vocabulary of words. We'll use:\n",
        "- \"low\" (appears 5 times)\n",
        "- \"lower\" (appears 2 times)\n",
        "- \"newest\" (appears 6 times)\n",
        "\n",
        "We'll represent each word as a sequence of characters separated by spaces, with '</w>' marking the end of a word."
      ],
      "metadata": {
        "id": "vQhNQ_DU5PXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial vocabulary\n",
        "vocab = {\n",
        "    'l o w </w>': 5,\n",
        "    'l o w e r </w>': 2,\n",
        "    'n e w e s t </w>': 6\n",
        "}\n",
        "\n",
        "print(\"Initial vocabulary:\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pbOvKWb1ZFW",
        "outputId": "88137d85-0816-43c7-fe53-36e7ea51a153"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial vocabulary:\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The BPE Process\n",
        "\n",
        "Now, let's apply BPE steps to see how it builds up subwords."
      ],
      "metadata": {
        "id": "hIej37z_5cgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 5\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "        break\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f\"\\nMerge #{i+1}: {best}\")\n",
        "    print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pedfbTJ95WPS",
        "outputId": "d087342f-b962-473d-8884-82bd3f4b8961"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Merge #1: ('w', 'e')\n",
            "{'l o w </w>': 5, 'l o we r </w>': 2, 'n e we s t </w>': 6}\n",
            "\n",
            "Merge #2: ('l', 'o')\n",
            "{'lo w </w>': 5, 'lo we r </w>': 2, 'n e we s t </w>': 6}\n",
            "\n",
            "Merge #3: ('n', 'e')\n",
            "{'lo w </w>': 5, 'lo we r </w>': 2, 'ne we s t </w>': 6}\n",
            "\n",
            "Merge #4: ('ne', 'we')\n",
            "{'lo w </w>': 5, 'lo we r </w>': 2, 'newe s t </w>': 6}\n",
            "\n",
            "Merge #5: ('newe', 's')\n",
            "{'lo w </w>': 5, 'lo we r </w>': 2, 'newes t </w>': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Just Happened?\n",
        "\n",
        "1. We started with individual characters.\n",
        "2. In each step, we found the most frequent pair of adjacent tokens.\n",
        "3. We created a new token by merging this pair.\n",
        "4. We repeated this process several times.\n",
        "\n",
        "## The Benefits\n",
        "\n",
        "- Common subwords emerge: Notice how 'es' and 'ew' became tokens. These are meaningful subword units!\n",
        "- Efficiency: We can now represent words with fewer tokens.\n",
        "- Flexibility: This method can handle new words by breaking them into learned subwords.\n",
        "\n",
        "## Real-world Application\n",
        "\n",
        "In practice, BPE is applied to much larger vocabularies and is a crucial preprocessing step for many language models. It helps these models handle large vocabularies efficiently and deal with rare or unseen words effectively."
      ],
      "metadata": {
        "id": "YK9kaRHV5ut8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test our BPE on a new word\n",
        "new_word = 'l o w e s t </w>'\n",
        "print(\"New word:\", new_word)\n",
        "\n",
        "for old, new in [('e s', 'es'), ('e w', 'ew')]:  # Apply our learned merges\n",
        "    new_word = new_word.replace(old, new)\n",
        "\n",
        "print(\"After applying BPE:\", new_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alc6Iu0r5bpk",
        "outputId": "4d96295d-e6e6-4435-d371-12fae01d39a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New word: l o w e s t </w>\n",
            "After applying BPE: l o w es t </w>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our simple BPE model was able to apply learned subword units to a new word it hadn't seen before!\n",
        "\n",
        "This is a simplified version of BPE, but it demonstrates the core concept. In real NLP applications, BPE is typically applied to much larger datasets and can learn thousands of merges, creating a rich vocabulary of subword units."
      ],
      "metadata": {
        "id": "tIgUozjk6ETf"
      }
    }
  ]
}