# Module 17: Advanced Applications

### Multimodal Systems: Text, Image, Audio, Video
- **Description**: Integrate multiple modalities for richer, interactive systems.
- **Concepts Covered**: `multimodal`, `vision-language`, `audio processing`, `video analysis`, `OCR`, `multimodal search`, `handwritten text recognition`, `long video processing`

- **Core Learning Resources**:
  - [![CLIP by OpenAI](https://badgen.net/badge/Website/CLIP_by_OpenAI/blue)](https://openai.com/research/clip)
  - [![Multimodal Transformers](https://badgen.net/badge/Paper/Multimodal_Transformers/purple)](https://arxiv.org/abs/2102.10765)
  - [![AWS Multimodal Search Tutorial](https://badgen.net/badge/Tutorial/AWS_Multimodal_Search_Tutorial/blue)](https://aws.amazon.com/developers)
  - [![Multimodal University](https://badgen.net/badge/Website/Multimodal_University/blue)](https://mixpeek.com/learn) - Comprehensive course on building production-ready multimodal AI systems

- **Vision Language Resources**:
  - [![DeepSeek-VL Paper](https://badgen.net/badge/Paper/DeepSeek-VL_Paper/purple)](https://arxiv.org/pdf/2403.05525) - State-of-the-art vision-language model
  - [![Imagine while Reasoning in Space](https://badgen.net/badge/Paper/Imagine_while_Reasoning_in_Space/purple)](https://arxiv.org/pdf/2501.07542) - Visual chain-of-thought reasoning
  - [![Qwen2.5VL Blog Post](https://badgen.net/badge/Blog/Qwen2.5VL_Blog_Post/cyan)](https://qwenlm.github.io/blog/qwen2.5-vl/) - Technical details of state-of-the-art model
  - [![Qwen2.5VL Fine-tuning Tutorial](https://badgen.net/badge/Tutorial/Qwen2.5VL_Fine-tuning_Tutorial/blue)](https://github.com/roboflow/notebooks)
  - [![Llama 3 Paper](https://badgen.net/badge/Paper/Llama_3_Paper/purple)](https://arxiv.org/pdf/2407.21783) - Meta's multimodal architecture
  - [![R1-V: RL for Visual Counting](https://badgen.net/badge/Github Repository/R1-V:_RL_for_Visual_Counting/gray)](https://github.com/Deep-Agent/R1-V)
  - [![MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://badgen.net/badge/Paper/MM1.5:_Methods,_Analysis_&_Insights_from_Multimodal_LLM_Fine-tuning/purple)](https://arxiv.org/abs/2409.20566) - Apple's comprehensive paper on:
  - [![BLIP-3 Paper](https://badgen.net/badge/Paper/BLIP-3_Paper/purple)](https://arxiv.org/pdf/2408.08872) - State-of-the-art multimodal model architecture and training methodology


- **OCR & Document Processing**:
  - [![HTR-VT: Handwritten Text Recognition](https://badgen.net/badge/Paper/HTR-VT:_Handwritten_Text_Recognition/purple)](https://arxiv.org/html/2409.08573v1)
  - [![HTR-VT Implementation](https://badgen.net/badge/Github Repository/HTR-VT_Implementation/gray)](https://github.com/YutingLi0606/HTR-VT)
  - [![PaliGemma2 Image to JSON Tutorial](https://badgen.net/badge/Colab Notebook/PaliGemma2_Image_to_JSON_Tutorial/orange)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb)
  - [![PaliGemma2 LaTeX OCR Tutorial](https://badgen.net/badge/Colab Notebook/PaliGemma2_LaTeX_OCR_Tutorial/orange)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb)

- **Vision Language Models**:
  - [![DeepSeek-VL](https://badgen.net/badge/Github Repository/DeepSeek-VL/gray)](https://github.com/deepseek-ai/DeepSeek-V)
  - [![Qwen2.5VL Models Collection](https://badgen.net/badge/Hugging Face Model/Qwen2.5VL_Models_Collection/yellow)](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5)
  - [![Qwen2.5VL Chat Interface](https://badgen.net/badge/Website/Qwen2.5VL_Chat_Interface/blue)](https://chat.qwenlm.ai)
  - [![Llama 3.2-Vision](https://badgen.net/badge/Hugging Face Model/Llama_3.2-Vision/yellow)](https://huggingface.co/meta-llama/llama-2-3.2b-vision)
  - [![SmolVLM Demo & Models](https://badgen.net/badge/Hugging Face Space/SmolVLM_Demo_&_Models/yellow)](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-256M-Demo)

- **OCR & Document Tools**:
  - [![Ollama-OCR](https://badgen.net/badge/Github Repository/Ollama-OCR/gray)](https://github.com/imanoop7/Ollama-OCR)
  - [![Surya](https://badgen.net/badge/Github Repository/Surya/gray)](https://github.com/VikParuchuri/surya) - Advanced OCR toolkit
  - [![DocTR](https://badgen.net/badge/Github Repository/DocTR/gray)](https://github.com/mindee/doctr)
  - [![PaddleOCR](https://badgen.net/badge/Github Repository/PaddleOCR/gray)](https://github.com/PaddlePaddle/PaddleOCR)

- **Datasets**:
  - [![Pixparse OCR Datasets](https://badgen.net/badge/Hugging Face Dataset/Pixparse_OCR_Datasets/yellow)](https://huggingface.co/collections/pixparse/pdf-document-ocr-datasets-660701430b0346f97c4bc628)
  - [![Pallet Load Manifest JSON Dataset](https://badgen.net/badge/Hugging Face Dataset/Pallet_Load_Manifest_JSON_Dataset/yellow)](https://universe.roboflow.com/roboflow-jvuqo/pallet-load-manifest-json)
  - [![Unsloth LaTeX OCR Dataset](https://badgen.net/badge/Hugging Face Dataset/Unsloth_LaTeX_OCR_Dataset/yellow)](https://universe.roboflow.com/roboflow-jvuqo/unsloth-latex-ocr)

### Code Generation & Code Repair
- **Description**: Use LLMs for code generation, debugging, and automated repair.
- **Concepts Covered**: `code generation`, `code repair`, `debugging`, `LLM`
- **Learning Resources**:
  - [![GitHub Copilot](https://badgen.net/badge/Website/GitHub_Copilot/blue)](https://github.com/features/copilot)
  - [![CodeBERT Paper](https://badgen.net/badge/Paper/CodeBERT_Paper/purple)](https://arxiv.org/abs/2002.09436)
- **Tools**:
  - [![VSCode Extensions](https://badgen.net/badge/Website/VSCode_Extensions/blue)](https://code.visualstudio.com/)
  - [![Tabnine](https://badgen.net/badge/Website/Tabnine/blue)](https://www.tabnine.com/)

### Intelligent Agents & Tool Integration
- **Description**: Build agents that integrate LLMs with external tools and APIs for automation.
- **Concepts Covered**: `intelligent agents`, `automation`, `tool integration`, `API interaction`
- **Learning Resources**:
  - [![Agent-Based Modeling](https://badgen.net/badge/Website/Agent-Based_Modeling/blue)](https://www.jasss.org/16/2/5.html)
  - [![LangChain Agents Guide](https://badgen.net/badge/Docs/LangChain_Agents_Guide/green)](https://python.langchain.com/docs/modules/agents/)
- **Tools**:
  - [![LangChain](https://badgen.net/badge/Github Repository/LangChain/gray)](https://github.com/hwchase17/langchain)
  - [![AutoGPT](https://badgen.net/badge/Github Repository/AutoGPT/gray)](https://github.com/Significant-Gravitas/Auto-GPT)

### Custom LLM Applications
- **Description**: Develop tailored LLM solutions for specific business or research needs.
- **Concepts Covered**: `custom applications`, `domain adaptation`, `specialized models`, `AI agents`, `RAG implementations`, `scalable solutions`
- **Learning Resources**:
  - [![Building Custom LLMs](https://badgen.net/badge/Tutorial/Building_Custom_LLMs/blue)](https://www.deeplearning.ai/short-courses/building-applications-with-vector-databases/)
  - [![Domain-Specific Language Models](https://badgen.net/badge/Paper/Domain-Specific_Language_Models/purple)](https://arxiv.org/abs/2004.06547)
  - [![Reflex LLM Examples](https://badgen.net/badge/Github Repository/Reflex_LLM_Examples/gray)](https://github.com/reflex-dev/reflex-llm-examples) - Curated repository of AI Apps showcasing practical LLM use cases
- **Tools**:
  - [![Hugging Face Transformers](https://badgen.net/badge/Hugging Face Model/Hugging_Face_Transformers/yellow)](https://huggingface.co/)
  - [![Custom Datasets](https://badgen.net/badge/Hugging Face Dataset/Custom_Datasets/yellow)](https://huggingface.co/docs/datasets/loading)
  - [![OpenSesame](https://badgen.net/badge/Website/OpenSesame/blue)](https://opensesame.dev/) - Custom LLM application development
  - [![Readwise](https://badgen.net/badge/Website/Readwise/blue)](https://readwise.io/) - Knowledge management and integration
  - [![Reflex](https://badgen.net/badge/Github Repository/Reflex/gray)](https://github.com/reflex-dev/reflex) - Framework for building AI applications

### Document Processing and Structured Data Extraction
- **Learning Resources**:
  - [![PDF Q&A with DeepSeek Tutorial](https://badgen.net/badge/Tutorial/PDF_Q&A_with_DeepSeek_Tutorial/blue)](https://youtube.com/watch?v=M6vZ6b75p9k&list=PLp01ObP3udmq2quR-RfrX4zNut_t_kNot) - Build a production-ready PDF Q&A chatbot using DeepSeek LLM and LangChain
  - [![Gemini PDF to Data Tutorial](https://badgen.net/badge/Tutorial/Gemini_PDF_to_Data_Tutorial/blue)](https://www.philschmid.de/gemini-pdf-to-data)
  - [![Gemini 2.0 File API Documentation](https://badgen.net/badge/Docs/Gemini_2.0_File_API_Documentation/green)](https://ai.google.dev/docs/file_api)
  - [![Pydantic Documentation](https://badgen.net/badge/Docs/Pydantic_Documentation/green)](https://docs.pydantic.dev/)
- **Tools**:
  - [![PDF Dino](https://badgen.net/badge/Website/PDF_Dino/blue)](https://pdfdino.com) - Neural network-based tool for extracting data, charts, and tables from PDFs with fast results (includes free tier)
  - [![Google Generative AI SDK](https://badgen.net/badge/Github Repository/Google_Generative_AI_SDK/gray)](https://github.com/google/generative-ai-python)
  - [![Pydantic](https://badgen.net/badge/Github Repository/Pydantic/gray)](https://github.com/pydantic/pydantic)
  - [![PyPDF2](https://badgen.net/badge/Github Repository/PyPDF2/gray)](https://pypdf2.readthedocs.io/)
  - [![Parsr](https://badgen.net/badge/Github Repository/Parsr/gray)](https://github.com/axa-group/Parsr) - Tool for transforming PDF, documents, and images into enriched structured data
- **Key Topics**:
  - Setting up the development environment
  - Managing API keys and clients
  - PDF upload and processing
  - Token count management
  - JSON schema definition with Pydantic
  - File size optimization strategies
  - Document structure extraction and enrichment

### AI Research Assistant Development
- **Learning Resources**:
  - [![Deep Research Agent Implementation](https://badgen.net/badge/Github Repository/Deep_Research_Agent_Implementation/gray)](https://github.com/dzhng/deep-research)