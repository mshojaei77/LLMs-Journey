
| Topics | Paper Title | Year Published |
|---|---|---|
| Supervised Learning, Fine-tuning, Transformers | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) | 2018 |
| Zero-Shot Learning, Generative Models | [Zero-shot Text Classification With Generative Language Models](https://arxiv.org/abs/1912.10165v1) | 2019 |
| Prompt Engineering, Few-Shot Learning | [Zero-shot Text Classification With Generative Language Models](https://arxiv.org/abs/1912.10165v1) | 2019 |
| Prompt Engineering, Knowledge Extraction | [PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction](https://arxiv.org/abs/2109.05190v3) | 2021 |
| Efficient Search, Contextualized Embeddings | [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832v2) | 2020 |
| Efficient Search, Late Interaction | [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488v3) | 2021 |
| Knowledge Retrieval, Generative Models | [General-Purpose Question-Answering with Macaw](https://arxiv.org/abs/2109.02593v1) | 2021 |
| Human Feedback, Reinforcement Learning | [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332v3) | 2021 |
| Sequence Modeling, Sparse Transformers | [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509v1) | 2019 |
| Long Documents, Attention Mechanisms | [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150v2) | 2020 |
| Few-Shot Learning | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) | 2020 |
| Data Scaling, Dataset Creation | [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027v1) | 2020 |
| Pre-training, Transfer Learning | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461v1) | 2019 |
| Model Parallelism, Large-Scale Training | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053v4) | 2019 |
| Attention Mechanisms, Transformers | [Attention Is All You Need](https://arxiv.org/abs/1706.03762v7) | 2017 |
| Pre-training Objectives, Transfer Learning | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) | 2018 |
| Instruction Tuning, Data and Methods | [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688) | 2023 |
| Fine-tuning, Few-Shot Learning | [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723v2) | 2020 |
| Zero-Shot Learning, Fine-tuning | [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652v5) | 2021 |
| Instruction Meta-Learning, Scaling | [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017) | 2022 |
| Emergent Abilities, Scaling | [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682v2) | 2022 |
| Scientific Applications, Knowledge Representation | [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) | 2022 |
| Open-Source, Efficiency | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1) | 2023 |
| Multimodal Learning, Perception | [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045v2) | 2023 |
| Domain-Specific, Finance | [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564v3) | 2023 |
| Training Data, Textbooks | [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644v2) | 2023 |
| Training Data, Textbooks, Technical Report | [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463v1) | 2023 |
| Progressive Learning, Scaling | [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415v1) | 2024 |
| Open-Source, Long-Term Scaling | [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954v1) | 2024 |
| Quantization, Efficiency | [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) | 2024 |
| Dataset Creation, Pre-training Data | [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159v2) | 2024 |
| Open-Source, Small Language Models | [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385v1) | 2024 |
| On-Device Deployment, Efficiency | [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) | 2024 |
| Open Training and Inference, Efficiency | [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](https://arxiv.org/abs/2404.14619) | 2024 |
| Low-Rank Adaptation, Fine-tuning | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685v2) | 2021 |
| Prompt Tuning, Continuous Prompts | [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190v1) | 2021 |
| Prompt Tuning, Scaling | [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691v2) | 2021 |
| Prompt Tuning, Few-Shot Learning | [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332v3) | 2021 |
| Prompt Tuning, Rules, Text Classification | [PTR: Prompt Tuning with Rules for Text Classification](https://arxiv.org/abs/2105.11259v3) | 2021 |
| Prompt Tuning, Knowledge Incorporation | [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://arxiv.org/abs/2108.02035v2) | 2021 |
| Kronecker Adapter, Efficiency | [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650v1) | 2022 |
| Adapters, Zero-init Attention | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199v2) | 2023 |
| Prompt Tuning, Multitask Learning, Transfer Learning | [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861v1) | 2023 |
| Adaptive Budget Allocation, LoRA | [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512v2) | 2023 |
| Residual Mapping, LoRA | [ResLoRA: Identity Residual Mapping in Low-Rank Adaption](https://arxiv.org/abs/2402.18039) | 2024 |
| Weight Decomposition, LoRA | [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) | 2024 |
| Memory Efficiency, Low-Rank Projection | [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) | 2024 |
| Serving, Scalability, LoRA | [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285v2) | 2023 |
| Quantization, LoRA | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659v4) | 2024 |
| Human Feedback, Alignment | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155v1) | 2022 |
| Browser Interaction, Human Feedback | [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332v3) | 2021 |
| Harmlessness, AI Feedback | [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073v1) | 2022 |
| Preference Optimization, Reward Models | [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) | 2023 |
| Helpfulness, Harmlessness, Open-Domain RL | [HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback](https://arxiv.org/abs/2403.08309v2) | 2024 |
| Workflow, Reward Modeling, Online RLHF | [RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/abs/2405.07863v1) | 2024 |
| Knowledge Transfer, Homologous Models | [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099v1) | 2023 |
| Knowledge Fusion | [Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491) | 2024 |
| Chat Models, Knowledge Fusion | [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2408.07990v1) | 2024 |
| Evolutionary Optimization, Recipes | [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187) | 2024 |
| Benchmarking, Holistic Evaluation | [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) | 2022 |
| Truthfulness, Factuality | [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958) | 2021 |
| Human Evaluation, Benchmarks | [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) | 2023 |
| Open Platform, Human Preference | [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132) | 2024 |
| Evaluation Methodology | [LLMEval: A Preliminary Study on How to Evaluate Large Language Models](https://arxiv.org/abs/2312.07398v2) | 2024 |
| Unified Library, Benchmarking | [PromptBench: A Unified Library for Evaluation of Large Language Models](https://arxiv.org/abs/2312.07910v2) | 2024 |
| Unified Framework, Multiple Datasets | [Catwalk: A Unified Language Model Evaluation Framework for Many Datasets](https://arxiv.org/abs/2312.10253v1) | 2024 |
| Reasoning Abilities, Cognitive Depth | [Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs](https://arxiv.org/abs/2312.17080v1) | 2024 |
| Few-Shot Learning, Prompt Design | [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://arxiv.org/abs/2108.04106v3) | 2021 |
| Few-Shot Learning, Differentiable Prompts | [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2108.13161v7) | 2021 |
| Data Labeling, GPT-3 | [Want To Reduce Labeling Cost? GPT-3 Can Help](https://arxiv.org/abs/2108.13487v1) | 2021 |
| Prompt Understanding, Analysis | [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247v2) | 2021 |
| Multilingual Models, Prompting | [Discrete and Soft Prompting for Multilingual Models](https://arxiv.org/abs/2109.03630v1) | 2021 |
| Aspect-Based Sentiment Analysis, Prompting | [Open Aspect Target Sentiment Classification with Natural Language Prompts](https://arxiv.org/abs/2109.03685v1) | 2021 |
| Few-Shot Learning, Inference Heuristics | [Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning](https://arxiv.org/abs/2109.04144v1) | 2021 |
| Few-Shot Learning, Task-Oriented Dialog Systems | [CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems](https://arxiv.org/abs/2109.04645v4) | 2021 |
| Few-Shot Learning, Grounded Dialog Generation | [Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation](https://arxiv.org/abs/2109.06513v2) | 2021 |
| Biomedical Knowledge, Language Models | [Can Language Models be Biomedical Knowledge Bases?](https://arxiv.org/abs/2109.07154v1) | 2021 |
| Multilingual Learning, Few-Shot Learning | [Language Models are Few-shot Multilingual Learners](https://arxiv.org/abs/2109.07684v1) | 2021 |
| Instruction Prompts, GPT-3 | [Reframing Instructional Prompts to GPTk's Language](https://arxiv.org/abs/2109.07830v3) | 2021 |
| Aspect-Based Sentiment Analysis, Prompt Tuning | [SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2109.08306v1) | 2021 |
| Commonsense Reasoning, Knowledge Prompting | [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387v3) | 2021 |
| Zero-Shot Task Generalization, Multitask Prompted Training | [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207v3) | 2021 |
| Chain-of-Thought, Reasoning | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903v6) | 2022 |
| Chain-of-Thought, Self-Consistency | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171v4) | 2022 |
| Chain-of-Thought, Patterns | [Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango](https://arxiv.org/abs/2209.07686v2) | 2022 |
| Chain-of-Thought, Automation | [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493v1) | 2022 |
| Multimodal Reasoning, Science Question Answering | [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513v2) | 2022 |
| Numerical Reasoning, Program of Thoughts | [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks](https://arxiv.org/abs/2211.12588v4) | 2022 |
| Chain-of-Thought, Empirical Study | [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001v2) | 2022 |
| Chain-of-Thought, Faithfulness | [Faithful Chain-of-Thought Reasoning](https://arxiv.org/abs/2301.13379v3) | 2023 |
| Contextual Relevance, Distractions | [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093v3) | 2023 |
| Chain-of-Thought, Active Prompting | [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246v3) | 2023 |
| Directional Stimulus Prompting, Guidance | [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520v4) | 2023 |
| Sentiment Analysis, Chain-of-Thought | [Reasoning Implicit Sentiment with Chain-of-Thought Prompting](https://arxiv.org/abs/2305.11255v4) | 2023 |
| Multilingual Reasoning, Chain-of-Thought | [Language Models are Multilingual Chain-of-Thought Reasoners](https://arxiv.org/abs/2210.03057v1) | 2023 |
| Reasoning, Acting, Language Models | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629v3) | 2023 |
| Summarization, Chain of Density Prompting | [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269v1) | 2023 |
| Chain-of-Thought, Streaming Batch | [Chain-of-Thought Prompting Under Streaming Batch: A Case Study](https://arxiv.org/abs/2306.00550v1) | 2024 |
| Chain-of-Thought, Conciseness | [The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models](https://arxiv.org/abs/2401.05618v1) | 2024 |
| Chain-of-Thought, Reasoning Step Length | [The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925v2) | 2024 |
| Chain-of-Thought, Table Understanding | [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://arxiv.org/abs/2401.04398v1) | 2024 |
| Consistency, Large Language Models | [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835v3) | 2024 |
| Retrieval Augmented Thoughts, Long-Horizon Generation | [RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](https://arxiv.org/abs/2403.05313) | 2024 |
| In-Context Learning, Mistakes | [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403) | 2024 |
| Abstraction, Reasoning | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117v1) | 2024 |
| Chain-of-Thought, Parallel Decoding | [Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster](https://arxiv.org/abs/2311.08263v1) | 2024 |
| Chain-of-Thought, Contrastive Learning | [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277v1) | 2024 |
| Code Generation, Prompt Engineering, Flow Engineering | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](https://arxiv.org/abs/2401.08500) | 2024 |
| Code Generation, Commonsense Reasoning | [Language Models of Code are Few-Shot Commonsense Learners](https://arxiv.org/abs/2210.07128v3) | 2022 |
| Code Generation, Program-Aided Language Models | [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435v2) | 2022 |
| Code Decompilation, Binary Code | [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://arxiv.org/abs/2403.05286v1) | 2023 |
| Web Navigation, Flexible Interfaces | [FLIN: A Flexible Natural Language Interface for Web Navigation](https://arxiv.org/abs/2010.12844v2) | 2020 |
| Few-Shot Text Classification, Label Identification | [Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification](https://arxiv.org/abs/2010.13641v1) | 2020 |
| Text Editing, Instruction Tuning | [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857) | 2023 |
| Query Expansion, Large Language Models | [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653) | 2023 |
| Complex Questions, Implicit Relations | [Inferring Implicit Relations in Complex Questions with Language Models](https://arxiv.org/abs/2204.13778v2) | 2022 |
| Agents, Tool Use, Planning | [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models](https://arxiv.org/abs/2212.04088v3) | 2022 |
| Robotic Task Planning, Large Language Models | [ProgPrompt: Generating Situated Robot Task Plans using Large Language Models](https://arxiv.org/abs/2209.11302v1) | 2022 |
| Few-Shot Dense Retrieval, Prompting | [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755v1) | 2022 |
| Tool Use, Multi-Step Reasoning | [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014v1) | 2023 |
| Multiple External Tools, Chain-of-Thought | [MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting](https://arxiv.org/abs/2305.16896v1) | 2023 |
| Declarative Language Model Calls, Pipelines | [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714) | 2023 |
| Hierarchical Agents, API Calls | [AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls](https://arxiv.org/abs/2402.04253) | 2024 |
| Vision and Language, Transfer Learning | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020v1) | 2021 |
| Zero-Shot Reasoning, Multimodal Reasoning | [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598v2) | 2022 |
| Multi-Agent Collaboration, Task Solving | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580v4) | 2023 |
| ChatGPT, Multimodal Reasoning, Action | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381v1) | 2023 |
| Tool Use, Multimodal Agents | [LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents](https://arxiv.org/abs/2311.05437v1) | 2023 |
| Efficient Multi-Modal Assistant, Small Language Model | [LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model](https://arxiv.org/abs/2401.02330v2) | 2024 |
| On-Device, Multimodal Capabilities | [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800v1) | 2024 |
| Vision Language Assistant, Mobile Devices | [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886v2) | 2024 |
| Multisensory, Object-Centric, Embodied Language Model | [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/abs/2401.08577) | 2024 |
| Unified Interface, Multi-Task Learning | [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478v3) | 2023 |
| Internet-Scale Knowledge, Embodied Agents | [MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://arxiv.org/abs/2206.08853v2) | 2022 |
| Federated Learning, Communication Efficiency | [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098v3) | 2021 |
| Pruning, One-Shot Pruning | [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774v3) | 2023 |
| Quantization, Efficient Fine-tuning | [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314v1) | 2023 |
| LoRA, Quantization | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659v4) | 2024 |
| Attention Mechanism, Efficiency | [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135v2) | 2022 |
| Attention Reuse, Low-Latency Inference | [Prompt Cache: Modular Attention Reuse for Low-Latency Inference](https://arxiv.org/abs/2311.04934v2) | 2024 |
| Data Deduplication, Language Model Improvement | [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499v2) | 2021 |
| Prompt Injection, Attack Techniques | [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527v1) | 2022 |
| Prompt Engineering, Jailbreaking | [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860v1) | 2023 |
| Persuasion, Jailbreaking, AI Safety | [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373v1) | 2024 |
| Jailbreak Defense, Intention Analysis | [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561v1) | 2024 |
| Deceptive Training, Safety Evasion | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566v3) | 2024 |
| Toxic Prompt Detection, Safety | [Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org/abs/2408.11727v1) | 2024 |
| Multi-Agent Collaboration, Task Solving | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580v4) | 2023 |
| Machine Learning Tasks, Automation | [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks](https://arxiv.org/abs/2304.14979v1) | 2023 |
| Meta Programming, Multi-Agent Collaboration | [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352v5) | 2023 |
| Task-Oriented Agents, Lightweight Library | [AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System](https://arxiv.org/abs/2402.15538v1) | 2024 |
| Automatic Agent Learning, Self-Planning | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning](https://arxiv.org/abs/2401.05268v2) | 2024 |
| Autonomous Language Agents, Open-Source | [Agents: An Open-source Framework for Autonomous Language Agents](https://arxiv.org/abs/2309.07870v3) | 2024 |
| Emergent Gaming Interaction | [MindAgent: Emergent Gaming Interaction](https://arxiv.org/abs/2309.09971v2) | 2024 |
| Automatic Agent Generation | [AutoAgents: A Framework for Automatic Agent Generation](https://arxiv.org/abs/2309.17288v2) | 2024 |
| Information-Seeking Agents, Large Language Models | [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models](https://arxiv.org/abs/2312.04889v3) | 2024 |
| Task-Based Agents, Memory, StrictJSON | [TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON](https://arxiv.org/abs/2407.15734v1) | 2024 |
| Collaborative Agents, Software Development, Agile Methodology | [AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology](https://arxiv.org/abs/2406.11912v2) | 2024 |
| Generalist Computer Agents, Self-Improvement | [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456) | 2024 |
| Medical Agents, Simulation | [Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents](https://arxiv.org/abs/2405.02957v1) | 2024 |
| Automated Exam Answering, Machine Learning Education | [From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams](https://arxiv.org/abs/2206.05442v7) | 2022 |
| Peer Review, Large Language Models | [ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing](https://arxiv.org/abs/2306.00622v1) | 2023 |
| Research Feedback, Large Language Models | [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](https://arxiv.org/abs/2310.01783) | 2023 |
| Research Idea Generation, Scientific Literature | [ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models](https://arxiv.org/abs/2404.07738) | 2024 |
| Automated Research, Scientific Papers | [Autonomous LLM-driven research from data to human-verifiable research papers](https://arxiv.org/abs/2404.17605v1) | 2024 |
| Peer Review Assistance, GPT-4 | [GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study](https://arxiv.org/abs/2307.05492) | 2024 |
| Accelerated Scientific Research, AI | [OpenResearcher: Unleashing AI for Accelerated Scientific Research](https://arxiv.org/abs/2408.06941v1) | 2024 |
| Scientific Discovery, GPT-4 | [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](https://arxiv.org/abs/2311.07361v2) | 2024 |
| Multilingual Reading Comprehension, Benchmark | [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https://arxiv.org/abs/2308.16884) | 2021 |
| Clinical Text Mining, Obesity Monitoring | [Evaluating ChatGPT text-mining of clinical records for obesity monitoring](https://arxiv.org/abs/2308.01666v1) | 2023 |
| Medical Question Answering, Scientific Literature | [Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature](https://arxiv.org/abs/2310.16146v1) | 2023 |
| Doctor-Patient Conversations, Clinical Notes | [NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes](https://arxiv.org/abs/2310.15959v2) | 2023 |
| Medical Question Answering, Expert-Level Performance | [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617) | 2023 |
| Patient Education, Discharge Instructions | [EHRTutor: Enhancing Patient Understanding of Discharge Instructions](https://arxiv.org/abs/2310.19212v1) | 2024 |
| Privacy Policy Analysis, Automation | [PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models](https://arxiv.org/abs/2309.10238v1) | 2024 |
| Medicine, Applications, Challenges | [A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges](https://arxiv.org/abs/2311.05112v2) | 2024 |
| Mobile Health Data, Behavioral Health Data | [From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models](https://arxiv.org/abs/2311.13063v2) | 2024 |
| Healthcare Assistants, Review | [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review](https://arxiv.org/abs/2311.01918v1) | 2024 |
| Occupation Extraction, Standardization | [LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models](https://arxiv.org/abs/2309.09708v2) | 2024 |
| Behavioral Assessment, LLM Therapists | [A Computational Framework for Behavioral Assessment of LLM Therapists](https://arxiv.org/abs/2401.00820v1) | 2024 |
| Psychological Applications, Review | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review](https://arxiv.org/abs/2401.01519v2) | 2024 |
| Multilingual Capabilities, Language Transfer | [LLaMA Beyond English: An Empirical Study on Language Capability Transfer](https://arxiv.org/abs/2401.01055v2) | 2024 |
| Turkish Language Models, Low-Resource Adaptation | [Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking](https://arxiv.org/abs/2405.04685v1) | 2024 |
| Turkish Language Models, Performance Comparison | [Performance Comparison of Turkish Language Models](https://arxiv.org/abs/2404.17010v1) | 2024 |
| Turkish Language Models, Monolingual Training | [Introducing cosmosGPT: Monolingual Training for Turkish Language Models](https://arxiv.org/abs/2404.17336v1) | 2024 |
| RAG vs Fine-tuning, Agriculture | [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406v2) | 2024 |
| Urban Planning, Language Models, Retrieval | [PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval](https://arxiv.org/abs/2402.19273) | 2024 |
| Multilingual Medical LLM, Democratization of Medical AI | [Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People](https://arxiv.org/abs/2403.03640) | 2024 |
| Medical Devices, Large Language Model | [SM70: A Large Language Model for Medical Devices](https://arxiv.org/abs/2312.06974v1) | 2023 |
| Zero-Shot Planning, Embodied Agents | [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207v2) | 2022 |
| Grounding Language, Robotic Affordances | [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691v2) | 2022 |
| Robotic Navigation, Pre-trained Models | [LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action](https://arxiv.org/abs/2207.04429v2) | 2022 |
| Embodied Reasoning, Planning, Language Models | [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608v1) | 2022 |
| Embodied Reasoning, Collaboration | [Collaborating with language models for embodied reasoning](https://arxiv.org/abs/2302.00763v1) | 2023 |
| Interactive Planning, Multi-Task Agents | [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://arxiv.org/abs/2302.01560v2) | 2023 |
| Conversational Agents, Memory, Fine-tuning | [From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models](https://arxiv.org/abs/2401.02777v1) | 2024 |
| Multi-Agent Collaboration, Intelligent Agents | [Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents](https://arxiv.org/abs/2306.03314) | 2024 |
| User Simulation, User Behavior Analysis | [When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm](https://arxiv.org/abs/2306.02552v2) | 2024 |
| Real-Time Interaction, Robotics | [Interactive Language: Talking to Robots in Real Time](https://arxiv.org/abs/2210.06407v1) | 2024 |
| On-Device Language Model, Super Agent | [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744) | 2024 |
| Embodied Agents, Instruction, Planning | [Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents](https://arxiv.org/abs/2305.02412v2) | 2024 |
| Personality Traits, Large Language Models | [PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits](https://arxiv.org/abs/2305.02547) | 2024 |
| Game Playing, Reasoning | [SPRING: Studying the Paper and Reasoning to Play Games](https://arxiv.org/abs/2305.15486v3) | 2024 |
| Reasoning, World Models | [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992v2) | 2024 |
| Multi-Agent Collaboration, Translation | [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804v1) | 2024 |
| Persona-Based Conversation, Dataset Generation | [Faithful Persona-based Conversational Dataset Generation with Large Language Models](https://arxiv.org/abs/2312.10007v1) | 2024 |
| Scaling, Efficiency | [Mixture of A Million Experts](https://arxiv.org/abs/2407.04153v1) | 2024 |
| Open-Source, Language Models | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | 2024 |
| Block Expansion, Progressive Learning | [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) | 2024 |
