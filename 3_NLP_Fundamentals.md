# Module 3: NLP Fundamentals

### Text Processing & Cleaning
- **Description**: Master techniques to clean and normalize raw text data.
- **Concepts Covered**: `text processing`, `data cleaning`, `normalization`, `tokenization`
- **Learning Resources**:
  - [Stanford NLP: Text Processing](https://nlp.stanford.edu/IR-book/html/htmledition/text-processing-1.html)
  - [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/)
- **Tools**:
  - [spaCy](https://spacy.io/)
  - [NLTK](https://www.nltk.org/)

### Word Embeddings & Contextual Representations
- **Description**: Learn to represent words as vectors to capture syntactic and semantic meaning.
- **Concepts Covered**: `word embeddings`, `Word2Vec`, `GloVe`, `contextual embeddings`
- **Learning Resources**:
  - [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
  - [GloVe Project](https://nlp.stanford.edu/projects/glove/)
- **Tools**:
  - [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)
  - [FastText](https://fasttext.cc/)

### Language Modeling Basics
- **Description**: Introduce statistical models for predicting the next word in a sequence.
- **Concepts Covered**: `language modeling`, `n-gram models`, `probabilistic models`, `next-word prediction`
- **Learning Resources**:
  - [N-Gram Language Modeling Guide](https://www.geeksforgeeks.org/n-gram-language-modeling/)
  - [Stanford CS224N: Natural Language Processing](https://web.stanford.edu/class/cs224n/)
  - [Stanford CS229: Machine Learning](https://cs229.stanford.edu/)
  - [Dense LLM Lecture](https://youtu.be/9vM4p9NN0Ts) - Comprehensive lecture with [slides](https://drive.google.com/file/d/1B46VFrqFAPAEj3kaCrBAtQqeh2_Ztawl/view?usp=sharing)
  - [Pre-training, Generative Models, Prompting and Alignment Book](https://arxiv.org/pdf/2501.09223) - 200+ page comprehensive guide
- **Tools**:
  - [NLTK](https://www.nltk.org/)
  - [KenLM](https://kheafield.com/code/kenlm/)