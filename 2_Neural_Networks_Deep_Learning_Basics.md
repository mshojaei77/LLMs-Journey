# Module 2: Neural Networks & Deep Learning Basics

### Coding & Algorithm Implementation
- **Description**: Master practical programming skills and algorithmic problem-solving essential for LLM development and optimization.
- **Concepts Covered**: `data structures`, `algorithms`, `problem solving`, `code optimization`, `Python programming`, `DSA concepts`
- **Learning Resources**:
  - [![70 LeetCode Problems Tutorial](https://badgen.net/badge/Video/70%20LeetCode%20Problems%20Tutorial/red)](https://www.youtube.com/watch?v=lvO88XxNAzs)

### Neural Network Fundamentals
- **Description**: Understand the building blocks of neural networks and deep learning.
- **Concepts Covered**: `neurons`, `layers`, `activation functions`, `backpropagation`, `gradient descent`, `loss functions`
- **Learning Resources**:
  - [![3Blue1Brown Neural Networks](https://badgen.net/badge/Video/3Blue1Brown%20Neural%20Networks/red)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
  - [![Deep Learning Fundamentals by Professor Bryce](https://badgen.net/badge/Video/Deep%20Learning%20Fundamentals/red)](https://www.youtube.com/playlist?list=PLgPbN3w-ia_PeT1_c5jiLW3RJdR7853b9)
  - [![Neural Networks from Scratch](https://badgen.net/badge/Book/Neural%20Networks%20from%20Scratch/purple)](https://nnfs.io/)
  - [![Deep Learning Book by Ian Goodfellow](https://badgen.net/badge/Book/Deep%20Learning%20Book/purple)](https://www.deeplearningbook.org/)

### Backpropagation & Gradient Descent
- **Description**: Learn the mechanism to update neural network parameters via error propagation.
- **Concepts Covered**: `backpropagation`, `gradient descent`, `loss functions`, `optimization`
- **Learning Resources**:
  - [![3Blue1Brown: Backpropagation](https://badgen.net/badge/Video/3Blue1Brown%3A%20Backpropagation/red)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
  - [![Micrograd by Karpathy](https://badgen.net/badge/Github%20Repository/Micrograd/gray)](https://github.com/karpathy/micrograd)
- **Tools**:
  - [![PyTorch Autograd](https://badgen.net/badge/Docs/PyTorch%20Autograd/green)](https://pytorch.org/docs/stable/autograd.html)
  - [![JAX Autodiff Cookbook](https://badgen.net/badge/Docs/JAX%20Autodiff%20Cookbook/green)](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)

### Neural Network Architectures
- **Description**: Survey various neural network structures fundamental to deep learning.
- **Concepts Covered**: `MLP`, `CNN`, `RNN`, `activation functions`
- **Learning Resources**:
  - [![Deep Learning Book](https://badgen.net/badge/Book/Deep%20Learning%20Book/purple)](https://www.deeplearningbook.org/)
  - [![Stanford CS231n](https://badgen.net/badge/Course/Stanford%20CS231n/orange)](http://cs231n.stanford.edu/)
  - [![Neural Networks: Zero to Hero by Karpathy](https://badgen.net/badge/Video/Neural%20Networks%3A%20Zero%20to%20Hero/red)](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
  - [![Building LLMs from scratch](https://badgen.net/badge/Video/Building%20LLMs%20from%20scratch/red)](https://youtube.com/playlist?list=your_playlist_id)
- **Tools**:
  - [![TensorFlow](https://badgen.net/badge/Framework/TensorFlow/green)](https://www.tensorflow.org/)

### Training Dynamics & Optimization Strategies
- **Description**: Understand elements that influence model training, including loss functions and learning rate schedules.
- **Concepts Covered**: `loss functions`, `optimization`, `learning rate scheduling`, `regularization`
- **Learning Resources**:
  - [![Optimizing Gradient Descent â€“ Sebastian Ruder](https://badgen.net/badge/Blog/Optimizing%20Gradient%20Descent/cyan)](https://ruder.io/optimizing-gradient-descent/)
  - [![CS231n: Optimization](https://badgen.net/badge/Course/CS231n%3A%20Optimization/orange)](http://cs231n.github.io/neural-networks-3/)
- **Tools**:
  - [![Weights & Biases](https://badgen.net/badge/Website/Weights%20%26%20Biases/blue)](https://wandb.ai/)
  - [![PyTorch Lightning](https://badgen.net/badge/Framework/PyTorch%20Lightning/green)](https://www.pytorchlightning.ai/)
