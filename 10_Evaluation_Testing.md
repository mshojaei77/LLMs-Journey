# Module 10: Evaluation & Testing

### Evaluation Metrics for LLMs
- **Description**: Measure LLM performance using standard metrics.
- **Concepts Covered**: `BLEU`, `ROUGE`, `perplexity`, `accuracy`
- **Learning Resources**:
  - [![Survey of Evaluation Metrics for NLG](https://badgen.net/badge/Paper/Survey%20of%20Evaluation%20Metrics%20for%20NLG/purple)](https://arxiv.org/abs/1612.09332)
  - [![Perplexity Explained](https://badgen.net/badge/Blog/Perplexity%20Explained/cyan)](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)
- **Tools**:
  - [![Hugging Face Evaluate](https://badgen.net/badge/Framework/Hugging%20Face%20Evaluate/green)](https://huggingface.co/docs/evaluate)
  - [![TensorBoard](https://badgen.net/badge/Framework/TensorBoard/green)](https://www.tensorflow.org/tensorboard)

### Benchmark Datasets & Leaderboards
- **Description**: Explore standardized benchmarks and leaderboards for evaluating LLM capabilities.
- **Concepts Covered**: `benchmarking`, `evaluation metrics`, `model comparison`, `capability assessment`
- **Learning Resources**:
  - [![GAIA Benchmark Paper](https://badgen.net/badge/Paper/GAIA%20Benchmark%20Paper/purple)](https://huggingface.co/spaces/gaia-benchmark/leaderboard)
  - [![GAIA Dataset](https://badgen.net/badge/Hugging%20Face%20Dataset/GAIA%20Dataset/yellow)](https://huggingface.co/datasets/gaia-benchmark/GAIA)
  - [![Hugging Face Leaderboards](https://badgen.net/badge/Website/Hugging%20Face%20Leaderboards/blue)](https://huggingface.co/spaces/leaderboard)
- **Tools**:
  - [![GAIA Benchmark](https://badgen.net/badge/Website/GAIA%20Benchmark/blue)](https://huggingface.co/spaces/gaia-benchmark/leaderboard) - Evaluates next-generation LLMs with augmented capabilities
  - [![Hugging Face Evaluate](https://badgen.net/badge/Framework/Hugging%20Face%20Evaluate/green)](https://huggingface.co/docs/evaluate)
  - [![EleutherAI Language Model Evaluation Harness](https://badgen.net/badge/Github%20Repository/EleutherAI%20Language%20Model%20Evaluation%20Harness/gray)](https://github.com/EleutherAI/lm-evaluation-harness)

### Bias, Fairness & Ethical Evaluation
- **Description**: Evaluate and mitigate biases in language models for equitable AI.
- **Concepts Covered**: `bias`, `fairness`, `ethical AI`, `model evaluation`
- **Learning Resources**:
  - [![Hugging Face Fairness Metrics](https://badgen.net/badge/Docs/Hugging%20Face%20Fairness%20Metrics/green)](https://huggingface.co/docs/evaluate/fairness_metrics)
  - [![Fairlearn Toolkit](https://badgen.net/badge/Website/Fairlearn%20Toolkit/blue)](https://fairlearn.org/)
- **Tools**:
  - [![Fairlearn](https://badgen.net/badge/Framework/Fairlearn/green)](https://fairlearn.org/)
  - [![CheckList](https://badgen.net/badge/Github%20Repository/CheckList/gray)](https://github.com/marcotcr/checklist)

### Custom Evaluation Frameworks
- **Description**: Develop tailored evaluation pipelines for specialized tasks.
- **Concepts Covered**: `custom evaluation`, `evaluation pipelines`, `benchmark datasets`
- **Learning Resources**:
  - [![LightEval Documentation](https://badgen.net/badge/Github%20Repository/LightEval%20Documentation/gray)](https://github.com/huggingface/lighteval)
  - [![EleutherAI Evaluation Harness](https://badgen.net/badge/Github%20Repository/EleutherAI%20Evaluation%20Harness/gray)](https://github.com/EleutherAI/lm-evaluation-harness)
- **Tools**:
  - [![LightEval](https://badgen.net/badge/Github%20Repository/LightEval/gray)](https://github.com/huggingface/lighteval)
  - [![EleutherAI Evaluation Harness](https://badgen.net/badge/Github%20Repository/EleutherAI%20Evaluation%20Harness/gray)](https://github.com/EleutherAI/lm-evaluation-harness)
