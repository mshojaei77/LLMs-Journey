# Module 10: Evaluation & Testing

### Evaluation Metrics for LLMs
- **Description**: Measure LLM performance using standard metrics.
- **Concepts Covered**: `BLEU`, `ROUGE`, `perplexity`, `accuracy`
- **Learning Resources**:
  - [Survey of Evaluation Metrics for NLG](https://arxiv.org/abs/1612.09332)
  - [Perplexity Explained](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)
- **Tools**:
  - [Hugging Face Evaluate](https://huggingface.co/docs/evaluate)
  - [TensorBoard](https://www.tensorflow.org/tensorboard)

### Benchmark Datasets & Leaderboards
- **Description**: Explore standardized benchmarks and leaderboards for evaluating LLM capabilities.
- **Concepts Covered**: `benchmarking`, `evaluation metrics`, `model comparison`, `capability assessment`
- **Learning Resources**:
  - [GAIA Benchmark Paper](https://huggingface.co/spaces/gaia-benchmark/leaderboard)
  - [GAIA Dataset](https://huggingface.co/datasets/gaia-benchmark/GAIA)
  - [Hugging Face Leaderboards](https://huggingface.co/spaces/leaderboard)
- **Tools**:
  - [GAIA Benchmark](https://huggingface.co/spaces/gaia-benchmark/leaderboard) - Evaluates next-generation LLMs with augmented capabilities
  - [Hugging Face Evaluate](https://huggingface.co/docs/evaluate)
  - [EleutherAI Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)

### Bias, Fairness & Ethical Evaluation
- **Description**: Evaluate and mitigate biases in language models for equitable AI.
- **Concepts Covered**: `bias`, `fairness`, `ethical AI`, `model evaluation`
- **Learning Resources**:
  - [Hugging Face Fairness Metrics](https://huggingface.co/docs/evaluate/fairness_metrics)
  - [Fairlearn Toolkit](https://fairlearn.org/)
- **Tools**:
  - [Fairlearn](https://fairlearn.org/)
  - [CheckList](https://github.com/marcotcr/checklist)

### Custom Evaluation Frameworks
- **Description**: Develop tailored evaluation pipelines for specialized tasks.
- **Concepts Covered**: `custom evaluation`, `evaluation pipelines`, `benchmark datasets`
- **Learning Resources**:
  - [LightEval Documentation](https://github.com/huggingface/lighteval)
  - [EleutherAI Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- **Tools**:
  - [LightEval](https://github.com/huggingface/lighteval)
  - [EleutherAI Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
